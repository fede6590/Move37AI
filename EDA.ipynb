{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook shows and run the web scrapping part of the processing of the raw data.\n",
    "The other part is called from python scripts (utils and preprocessing) so it don't goes to long about DF manipulations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, libraries & reading raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTING FUNCTIONS FROM SCRIPTS ###\n",
    "from preprocessing import game_data_pp_1, game_data_pp_2, game_data_pp_3, nfl_elo_pp_1, nfl_elo_pp_2, nfl_pp\n",
    "from utils import to_pickle, from_pickle, search_teams_1, search_teams_2\n",
    "\n",
    "### LIBRARIES\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "# import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = pd.read_csv('raw_data/game_data.csv', index_col=0)\n",
    "nfl_elo = pd.read_csv('raw_data/nfl_elo.csv')\n",
    "nfl = pd.read_excel('raw_data/nfl.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAME_DATA FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(game_data)\n",
    "print(list(game_data.columns))\n",
    "type(game_data['date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = game_data_pp_1(game_data)\n",
    "\n",
    "display(game_data)\n",
    "print(list(game_data.columns))\n",
    "type(game_data['date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('pickles/TEAMS.pkl'):\n",
    "\n",
    "    teams = {}\n",
    "    links = game_data['link']\n",
    "\n",
    "    ### Looping on game_date links, extracting abbreviations from the link and scrapping the full name on the web page.\n",
    "    ### Every time it founds a new abbreviation, a new key/value pair is generated and add to the teams dictionary.\n",
    "    for i, link in tqdm(enumerate(links), total=len(links)):\n",
    "        URL = link\n",
    "        pointer = 'teams' # Word just before 3 letter team abbreviation in this link format\n",
    "        j = link.find(pointer)\n",
    "        team = link[j+len(pointer)+1 : j+len(pointer)+4].upper() # Extracting abbreviation\n",
    "\n",
    "        if i > 0 and team == list(teams)[-1]: # Evoid to repeat the process if the team is already added to the dict\n",
    "            continue\n",
    "        else:\n",
    "            r = requests.get(URL)\n",
    "            parsed = r.text\n",
    "            loc = parsed.find('meta name=\"Description\" content')\n",
    "\n",
    "            if loc == -1: # If IP is banned from the server, parsed will not have the needed content: use proxy\n",
    "                print('Banned IP: use proxy or wait before trying again')\n",
    "                os.remove('pickles/TEAMS.pkl')\n",
    "                break\n",
    "            \n",
    "            loc = loc + 52\n",
    "            full_name = parsed[loc:loc+50].split(',', 1)[0]\n",
    "            full_name = full_name[:-7]\n",
    "\n",
    "            teams[team] = full_name # Adding new key/value pair to dict\n",
    "\n",
    "        time.sleep(3) # Sleep to evoid being banned\n",
    "\n",
    "    os.makedirs('pickles', exist_ok=True)\n",
    "    to_pickle(teams, filename='pickles/TEAMS.pkl')\n",
    "\n",
    "    del links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = from_pickle('pickles/TEAMS.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = game_data_pp_2(game_data, teams)\n",
    "\n",
    "display(game_data)\n",
    "print(list(game_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('pickles/AWAY.pkl'):\n",
    "\n",
    "    game_data_parsing = game_data[['Season', 'Team', 'Link']]\n",
    "\n",
    "    away = []\n",
    "    for i, row in tqdm(game_data_parsing.iterrows(), total = game_data_parsing.shape[0]):\n",
    "        link = row['Link']\n",
    "\n",
    "        if (i > 0) and (link == link_last):\n",
    "            continue\n",
    "        else:\n",
    "            URL = link + '#games'\n",
    "            season = row['Season']\n",
    "            team = row['Team']\n",
    "            table_link = requests.get(URL).content\n",
    "            df_list = pd.read_html(table_link)\n",
    "            df_bis = df_list[1]\n",
    "            df_bis.columns = df_bis.columns.droplevel()\n",
    "            df_bis = df_bis[['Date', 'Unnamed: 8_level_1']]\n",
    "            try:\n",
    "                cut_playoffs = df_bis.index[df_bis['Date'] == 'Playoffs'].to_list()[0]\n",
    "                df_bis = df_bis.iloc[:cut_playoffs, :]\n",
    "            except:\n",
    "                pass\n",
    "            away_set = list(df_bis['Unnamed: 8_level_1'].values)\n",
    "\n",
    "            gd_temp = game_data[game_data['Team'] == team]\n",
    "            gd_temp = gd_temp[gd_temp['Season'] == season]\n",
    "\n",
    "            if len(away_set) != gd_temp.shape[0]:\n",
    "                raise ValueError(f'Lenghts are not matching for {team} {season} ({link}): {len(away_set)} vs {gd_temp.shape[0]}')\n",
    "\n",
    "            link_last = link\n",
    "            away += away_set\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        os.makedirs('pickles', exist_ok=True)\n",
    "        to_pickle(away, 'pickles/AWAY.pkl')\n",
    "\n",
    "else:\n",
    "    away = from_pickle('pickles/AWAY.pkl')\n",
    "\n",
    "game_data.insert(3, 'Away?', away)\n",
    "\n",
    "del away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = game_data_pp_3(game_data)\n",
    "\n",
    "display(game_data)\n",
    "print(list(game_data.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFL_ELO FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(nfl_elo)\n",
    "print(list(nfl_elo.columns))\n",
    "type(nfl_elo['date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_elo, teams2add, nfl_elo_sub = nfl_elo_pp_1(nfl_elo, sub_num=10)\n",
    "\n",
    "display(nfl_elo)\n",
    "print(type(nfl_elo['date'][0]))\n",
    "\n",
    "display(nfl_elo_sub)\n",
    "\n",
    "temp = teams2add - set(list(teams.keys()))\n",
    "print(f'The are {len(temp)} abbreviations without full names')\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('pickles/TEAMS_final.pkl'):\n",
    "\n",
    "    nfl_elo_parsing = nfl_elo_sub[['date', 'season', 'team1', 'team2']]\n",
    "\n",
    "    for _, row in tqdm(nfl_elo_parsing.iterrows(), total = nfl_elo_parsing.shape[0]):\n",
    "        if len(temp) == 0:\n",
    "            break\n",
    "        \n",
    "        date = row['date']\n",
    "        season = row['season']\n",
    "        team1_i = row['team1']\n",
    "        team2_i = row['team2']\n",
    "        \n",
    "        if (team1_i and team2_i) in teams:\n",
    "            continue\n",
    "\n",
    "        if team1_i not in teams:\n",
    "            search_teams_1(season, team1_i, teams)\n",
    "\n",
    "        if team2_i not in teams:\n",
    "            search_teams_1(season, team2_i, teams)\n",
    "\n",
    "        if (team1_i or team2_i) not in teams:\n",
    "            search_teams_2(date, team1_i, team2_i, teams)\n",
    "\n",
    "    os.makedirs('pickles', exist_ok=True)\n",
    "    to_pickle(teams, 'pickles/TEAMS_final.pkl')\n",
    "\n",
    "    del nfl_elo_parsing\n",
    "\n",
    "else:\n",
    "    teams = from_pickle('pickles/TEAMS_final.pkl')\n",
    "\n",
    "temp = teams2add - set(list(teams.keys()))\n",
    "print(f'{len(temp)} team names (full and abbreviated) still missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_elo = nfl_elo_pp_2(nfl_elo, teams)\n",
    "\n",
    "display(nfl_elo)\n",
    "print(list(nfl_elo.columns))\n",
    "\n",
    "temp = teams2add - set(list(teams.keys()))\n",
    "print(f'The are {len(temp)} abbreviations without full names')\n",
    "del temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFL FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(nfl)\n",
    "print(list(nfl.columns))\n",
    "type(nfl['Date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl = nfl_pp(nfl, teams)\n",
    "\n",
    "display(nfl)\n",
    "print(list(nfl.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(game_data.columns))\n",
    "print(list(nfl.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = ['Date', 'Home Team', 'Away Team']\n",
    "merge = pd.merge(game_data, nfl, how='outer', on=merge_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "439271dc3f168a0ce2055a4353efd4df0f31329f275977c8192fe016291bd4f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
